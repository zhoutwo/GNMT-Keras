{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouz2/.conda/envs/rbenv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Conv2D, Flatten, MaxPooling2D, Reshape, Activation, Embedding, TimeDistributed\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('reuters.csv', delimiter='\\t', names=['id', 'timestamp', 'title', 'url', 'first_line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 10000  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = min(2, len(df))  # Number of samples to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = df['first_line'][:num_samples]\n",
    "output_texts = df['title'][:num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = input_texts.apply(lambda i: i.strip().lower() + ' \\n')\n",
    "output_texts = output_texts.apply(lambda o: '\\t '+ o.strip().lower() + ' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~', oov_token='<unk>')\n",
    "tokenizer.fit_on_texts(input_texts)\n",
    "tokenizer.fit_on_texts(output_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
    "output_sequences = tokenizer.texts_to_sequences(output_texts)\n",
    "word_id_dict = tokenizer.word_index\n",
    "id_word_dict = dict()\n",
    "for k in word_id_dict:\n",
    "    id_word_dict[word_id_dict[k]] = k;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(input_texts)\n",
    "max_encoder_seq_length = max([len(seq) for seq in input_sequences])\n",
    "max_decoder_seq_length = max([len(seq) for seq in output_sequences])\n",
    "num_dict_size = len(tokenizer.word_index)\n",
    "# num_input_tokens = len(input_dict)\n",
    "# num_output_tokens = len(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2\n",
      "Max sequence length for inputs: 42\n",
      "Max sequence length for outputs: 11\n",
      "Number of words in the dictionary (including OOV token): 73\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', num_samples)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "# print('Num of tokens for inputs:', num_input_tokens)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "# print('Num of tokens for outputs:', num_output_tokens)\n",
    "print('Number of words in the dictionary (including OOV token):', num_dict_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input_data = np.zeros((num_samples, max_encoder_seq_length, num_input_tokens),dtype='float32')\n",
    "# reversed_encoder_input_data = np.zeros((num_samples, max_encoder_seq_length, num_input_tokens),dtype='float32')\n",
    "# decoder_input_data = np.zeros((num_samples, max_decoder_seq_length, num_output_tokens),dtype='float32')\n",
    "# decoder_target_data = np.zeros((num_samples, max_decoder_seq_length, num_output_tokens),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(num_samples):\n",
    "#     input_text = input_tokenizer.texts_to_sequences(input_texts[i])\n",
    "#     input_text = input_dict\n",
    "#     encoder_input_data[i, :, :] = input_tokenizer.texts_to_sequences(input_texts[i])\n",
    "#     reversed_encoder_input_data[i, :, :] = encoder_input_data[i, ::-1, :]\n",
    "#     decoder_input_data[i, :, :] = output_tokenizer.texts_to_sequences(output_texts[i])\n",
    "#     decoder_target_data[i, :, :] = np.row(decoder_input_data[i, :, :], 1)\n",
    "#     decoder_target_data[i, 0, :] = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 42)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 42, 256)      18688       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 11, 256)      18688       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 256), (None, 525312      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 11, 256), (N 525312      embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2816)         0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 11)           30987       flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,118,987\n",
      "Trainable params: 1,118,987\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Baseline model: 1 LSTM encoder + 1 LSTM decoder\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(max_encoder_seq_length,))\n",
    "# print(encoder_inputs.shape)\n",
    "encoder_embedding = Embedding(num_dict_size, latent_dim)\n",
    "encoder = encoder_embedding(encoder_inputs)\n",
    "# print(encoder.shape)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_decoder_seq_length,))\n",
    "decoder_embedding = Embedding(num_dict_size, latent_dim)\n",
    "decoder = decoder_embedding(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
    "decoder = Flatten()(decoder)\n",
    "decoder_dense = Dense(max_decoder_seq_length, activation='relu')\n",
    "decoder_outputs = decoder_dense(decoder)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)# Compile & run training\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.summary()\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((num_samples, max_encoder_seq_length), dtype='int32')\n",
    "decoder_input_data = np.zeros((num_samples, max_decoder_seq_length), dtype='int32')\n",
    "decoder_target_data = np.zeros((num_samples, max_decoder_seq_length), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(num_samples):\n",
    "    encoder_input_data[k][:len(input_sequences[k])] = input_sequences[k][:]\n",
    "    decoder_input_data[k][:len(output_sequences[k])] = output_sequences[k][:]\n",
    "    decoder_target_data[k][:len(output_sequences[k])-1] = output_sequences[k][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_input_data = encoder_input_data.reshape((*encoder_input_data.shape, 1))\n",
    "# decoder_input_data = decoder_input_data.reshape((*decoder_input_data.shape, 1))\n",
    "# decoder_target_data = decoder_target_data.reshape((*decoder_target_data.shape, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "2/2 [==============================] - 3s 1s/step - loss: 966.4792\n",
      "Epoch 2/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 966.0231\n",
      "Epoch 3/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 964.4863\n",
      "Epoch 4/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 962.2003\n",
      "Epoch 5/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 958.8226\n",
      "Epoch 6/10000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 953.3760\n",
      "Epoch 7/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 943.4501\n",
      "Epoch 8/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 921.7133\n",
      "Epoch 9/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 858.6315\n",
      "Epoch 10/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 718.5693\n",
      "Epoch 11/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 642.2038\n",
      "Epoch 12/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 596.7671\n",
      "Epoch 13/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 549.6208\n",
      "Epoch 14/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 500.9847\n",
      "Epoch 15/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 453.4932\n",
      "Epoch 16/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 409.2786\n",
      "Epoch 17/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 372.8833\n",
      "Epoch 18/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 349.6808\n",
      "Epoch 19/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 340.3130\n",
      "Epoch 20/10000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 340.3723\n",
      "Epoch 21/10000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 343.7001\n",
      "Epoch 22/10000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 346.6462\n",
      "Epoch 23/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 348.3558\n",
      "Epoch 24/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 348.0207\n",
      "Epoch 25/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 348.4865\n",
      "Epoch 26/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 347.6675\n",
      "Epoch 27/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 345.6861\n",
      "Epoch 28/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 343.8582\n",
      "Epoch 29/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 342.2830\n",
      "Epoch 30/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 340.2329\n",
      "Epoch 31/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 338.6975\n",
      "Epoch 32/10000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 337.4311\n",
      "Epoch 33/10000\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 336.3290\n",
      "Epoch 34/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 335.0042\n",
      "Epoch 35/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 334.5878\n",
      "Epoch 36/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 333.7387\n",
      "Epoch 37/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 333.7659\n",
      "Epoch 38/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 333.8006\n",
      "Epoch 39/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 333.4227\n",
      "Epoch 40/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 333.3412\n",
      "Epoch 41/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 332.3734\n",
      "Epoch 42/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 331.6927\n",
      "Epoch 43/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 330.4498\n",
      "Epoch 44/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 329.6117\n",
      "Epoch 45/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 328.3401\n",
      "Epoch 46/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 327.5296\n",
      "Epoch 47/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 326.3158\n",
      "Epoch 48/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 325.4613\n",
      "Epoch 49/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 324.2087\n",
      "Epoch 50/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 323.3080\n",
      "Epoch 51/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 321.7941\n",
      "Epoch 52/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 320.4691\n",
      "Epoch 53/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 318.8575\n",
      "Epoch 54/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 317.0320\n",
      "Epoch 55/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 316.0949\n",
      "Epoch 56/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 313.4162\n",
      "Epoch 57/10000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 311.8924\n",
      "Epoch 58/10000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 309.9120\n",
      "Epoch 59/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 306.6795\n",
      "Epoch 60/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 306.6232\n",
      "Epoch 61/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 302.1804\n",
      "Epoch 62/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 299.8348\n",
      "Epoch 63/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 297.4319\n",
      "Epoch 64/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 294.0970\n",
      "Epoch 65/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 290.4088\n",
      "Epoch 66/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 286.8460\n",
      "Epoch 67/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 283.4826\n",
      "Epoch 68/10000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 279.1336\n",
      "Epoch 69/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 275.4348\n",
      "Epoch 70/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 271.2962\n",
      "Epoch 71/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 266.5933\n",
      "Epoch 72/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 262.0599\n",
      "Epoch 73/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 257.5678\n",
      "Epoch 74/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 252.2692\n",
      "Epoch 75/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 247.8177\n",
      "Epoch 76/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 243.0379\n",
      "Epoch 77/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 237.6017\n",
      "Epoch 78/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 231.9012\n",
      "Epoch 79/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 225.8138\n",
      "Epoch 80/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 220.1589\n",
      "Epoch 81/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 213.6041\n",
      "Epoch 82/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 207.3492\n",
      "Epoch 83/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 201.1876\n",
      "Epoch 84/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 194.8673\n",
      "Epoch 85/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 188.3271\n",
      "Epoch 86/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 180.7866\n",
      "Epoch 87/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 174.1040\n",
      "Epoch 88/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 167.3417\n",
      "Epoch 89/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 160.5186\n",
      "Epoch 90/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 153.8394\n",
      "Epoch 91/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 146.1736\n",
      "Epoch 92/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 139.2598\n",
      "Epoch 93/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 132.3440\n",
      "Epoch 94/10000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 125.2016\n",
      "Epoch 95/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 118.3146\n",
      "Epoch 96/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 111.5474\n",
      "Epoch 97/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 105.0301\n",
      "Epoch 98/10000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 98.6178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 92.6249\n",
      "Epoch 100/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 86.6923\n",
      "Epoch 101/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 80.1432\n",
      "Epoch 102/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 74.2387\n",
      "Epoch 103/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 69.1123\n",
      "Epoch 104/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 64.2582\n",
      "Epoch 105/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 59.6224\n",
      "Epoch 106/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 55.0822\n",
      "Epoch 107/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 50.7098\n",
      "Epoch 108/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 46.5124\n",
      "Epoch 109/10000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 42.5043\n",
      "Epoch 110/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 38.7117\n",
      "Epoch 111/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 35.2542\n",
      "Epoch 112/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 32.1314\n",
      "Epoch 113/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 29.2626\n",
      "Epoch 114/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 26.5749\n",
      "Epoch 115/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 23.9147\n",
      "Epoch 116/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 21.5005\n",
      "Epoch 117/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 19.2803\n",
      "Epoch 118/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 17.2181\n",
      "Epoch 119/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 15.3122\n",
      "Epoch 120/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 13.5824\n",
      "Epoch 121/10000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 12.0207\n",
      "Epoch 122/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 10.5955\n",
      "Epoch 123/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 9.2861\n",
      "Epoch 124/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 8.0959\n",
      "Epoch 125/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 7.0428\n",
      "Epoch 126/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 6.1153\n",
      "Epoch 127/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.2858\n",
      "Epoch 128/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.5483\n",
      "Epoch 129/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 3.9068\n",
      "Epoch 130/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 3.3404\n",
      "Epoch 131/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 2.8359\n",
      "Epoch 132/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 2.4118\n",
      "Epoch 133/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 2.0876\n",
      "Epoch 134/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 1.7863\n",
      "Epoch 135/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 1.5324\n",
      "Epoch 136/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 1.3216\n",
      "Epoch 137/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 1.1459\n",
      "Epoch 138/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.9998\n",
      "Epoch 139/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.8783\n",
      "Epoch 140/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.7775\n",
      "Epoch 141/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.6951\n",
      "Epoch 142/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.6283\n",
      "Epoch 143/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.5739\n",
      "Epoch 144/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.5339\n",
      "Epoch 145/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.5057\n",
      "Epoch 146/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4852\n",
      "Epoch 147/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4677\n",
      "Epoch 148/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4534\n",
      "Epoch 149/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4421\n",
      "Epoch 150/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4331\n",
      "Epoch 151/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4254\n",
      "Epoch 152/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4191\n",
      "Epoch 153/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4152\n",
      "Epoch 154/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4141\n",
      "Epoch 155/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4142\n",
      "Epoch 156/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4149\n",
      "Epoch 157/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4174\n",
      "Epoch 158/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4213\n",
      "Epoch 159/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4246\n",
      "Epoch 160/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4261\n",
      "Epoch 161/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4266\n",
      "Epoch 162/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4268\n",
      "Epoch 163/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4263\n",
      "Epoch 164/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4249\n",
      "Epoch 165/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4229\n",
      "Epoch 166/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4208\n",
      "Epoch 167/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4185\n",
      "Epoch 168/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4165\n",
      "Epoch 169/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4150\n",
      "Epoch 170/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4144\n",
      "Epoch 171/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4141\n",
      "Epoch 172/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4136\n",
      "Epoch 173/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4130\n",
      "Epoch 174/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4125\n",
      "Epoch 175/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4122\n",
      "Epoch 176/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4119\n",
      "Epoch 177/10000\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 0.4114\n",
      "Epoch 178/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4110\n",
      "Epoch 179/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4108\n",
      "Epoch 180/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4107\n",
      "Epoch 181/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4107\n",
      "Epoch 182/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4105\n",
      "Epoch 183/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4104\n",
      "Epoch 184/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4102\n",
      "Epoch 185/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4101\n",
      "Epoch 186/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4099\n",
      "Epoch 187/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4098\n",
      "Epoch 188/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4097\n",
      "Epoch 189/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4097\n",
      "Epoch 190/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4097\n",
      "Epoch 191/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4096\n",
      "Epoch 192/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4096\n",
      "Epoch 193/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4096\n",
      "Epoch 194/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4095\n",
      "Epoch 195/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4095\n",
      "Epoch 196/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4094\n",
      "Epoch 197/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4093\n",
      "Epoch 199/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4092\n",
      "Epoch 200/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4092\n",
      "Epoch 201/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4092\n",
      "Epoch 202/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4093\n",
      "Epoch 203/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4093\n",
      "Epoch 204/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4093\n",
      "Epoch 205/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4092\n",
      "Epoch 206/10000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4092\n",
      "Epoch 207/10000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4092\n",
      "Epoch 208/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4092\n",
      "Epoch 209/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4092\n",
      "Epoch 210/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 211/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 212/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 213/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 214/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4092\n",
      "Epoch 215/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 216/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 217/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 218/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 219/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 220/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 221/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 222/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 223/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 224/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 225/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 226/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 227/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 228/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 229/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 230/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 231/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 232/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 233/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 234/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 235/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 236/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4091\n",
      "Epoch 237/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 238/10000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4091\n",
      "Epoch 239/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 240/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 241/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 242/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 243/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 244/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 245/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 246/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 247/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 248/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 249/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 250/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 251/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 252/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 253/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 254/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 255/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 256/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 257/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 258/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 259/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 260/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 261/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 262/10000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4091\n",
      "Epoch 263/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 264/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 265/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 266/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4091\n",
      "Epoch 267/10000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4091\n",
      "Epoch 268/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 269/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 270/10000\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 0.4091\n",
      "Epoch 271/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 272/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 273/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 274/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 275/10000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4091\n",
      "Epoch 276/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 277/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 278/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 279/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 280/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 281/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 282/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 283/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 284/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 285/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 286/10000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4091\n",
      "Epoch 287/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 288/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 289/10000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4091\n",
      "Epoch 290/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 291/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 292/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 293/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 294/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 295/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 296/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 298/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 299/10000\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 0.4091\n",
      "Epoch 300/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 301/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 302/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 303/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 304/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 305/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 306/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 307/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 308/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 309/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 310/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 311/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 312/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 313/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 314/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 315/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4091\n",
      "Epoch 316/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 317/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 318/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 319/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 320/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 321/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 322/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 323/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 324/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 325/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 326/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 327/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4091\n",
      "Epoch 328/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 329/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 330/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 331/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 332/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 333/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 334/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 335/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 336/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 337/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 338/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 339/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 340/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 341/10000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4091\n",
      "Epoch 342/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 343/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 344/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 345/10000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4091\n",
      "Epoch 346/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 347/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4091\n",
      "Epoch 348/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 349/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 350/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 351/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 352/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 353/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 354/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 355/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 356/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 357/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 358/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 359/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 360/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 361/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 362/10000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4091\n",
      "Epoch 363/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 364/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 365/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 366/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 367/10000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4091\n",
      "Epoch 368/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 369/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 370/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 371/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 372/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 373/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 374/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 375/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 376/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 377/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 378/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 379/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 380/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 381/10000\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 0.4091\n",
      "Epoch 382/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 383/10000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4091\n",
      "Epoch 384/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 385/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 386/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4091\n",
      "Epoch 387/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 388/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 389/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 390/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 391/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 392/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 393/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 394/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 395/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 397/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4091\n",
      "Epoch 398/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 399/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 400/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 401/10000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4091\n",
      "Epoch 402/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 403/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 404/10000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4091\n",
      "Epoch 405/10000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4091\n",
      "Epoch 406/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 407/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 408/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 409/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 410/10000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4091\n",
      "Epoch 411/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 412/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 413/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 414/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 415/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 416/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 417/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 418/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 419/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 420/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 421/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 422/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 423/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 424/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 425/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 426/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 427/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 428/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 429/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 430/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 431/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 432/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 433/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 434/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 435/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 436/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 437/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 438/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 439/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 440/10000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4091\n",
      "Epoch 441/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 442/10000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4091\n",
      "Epoch 443/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 444/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 445/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 446/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 447/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 448/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 449/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 450/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 451/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 452/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 453/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 454/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 455/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 456/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 457/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 458/10000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4091\n",
      "Epoch 459/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 460/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 461/10000\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.4091\n",
      "Epoch 462/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 463/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 464/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 465/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 466/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 467/10000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4091\n",
      "Epoch 468/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 469/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4091\n",
      "Epoch 470/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4091\n",
      "Epoch 471/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 472/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 473/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 474/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 475/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 476/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 477/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 478/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 479/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 480/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 481/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 482/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 483/10000\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.4091\n",
      "Epoch 484/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 485/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 486/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 487/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 488/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 489/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 490/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 491/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 492/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 493/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 494/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 495/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 496/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4091\n",
      "Epoch 497/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 498/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 499/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 500/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 501/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 502/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 503/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 504/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 505/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 506/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 507/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4091\n",
      "Epoch 508/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 509/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 510/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 511/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 512/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 513/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 514/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 515/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 516/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 517/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4091\n",
      "Epoch 518/10000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4091\n",
      "Epoch 519/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 520/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 521/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 522/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 523/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 524/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 525/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 526/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 527/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 528/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 529/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 530/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 531/10000\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 0.4091\n",
      "Epoch 532/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 533/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 534/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 535/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 536/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 537/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 538/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 539/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 540/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 541/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 542/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 543/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 544/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 545/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 546/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 547/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 548/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 549/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 550/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 551/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 552/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 553/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 554/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4091\n",
      "Epoch 555/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 556/10000\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 0.4091\n",
      "Epoch 557/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 558/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 559/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 560/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 561/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 562/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 563/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 564/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 565/10000\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 0.4091\n",
      "Epoch 566/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 567/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 568/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 569/10000\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 0.4091\n",
      "Epoch 570/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 571/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 572/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 573/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 574/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 575/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 576/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 577/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 578/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 579/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 580/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 581/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 582/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 583/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 584/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 585/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 586/10000\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 0.4091\n",
      "Epoch 587/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 588/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 589/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 590/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 591/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 592/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 593/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 594/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 595/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 596/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 597/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 598/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 599/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 600/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 601/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 602/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 603/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 604/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 605/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 606/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 607/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 608/10000\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 0.4091\n",
      "Epoch 609/10000\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.4091\n",
      "Epoch 610/10000\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.4091\n",
      "Epoch 611/10000\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 0.4091\n",
      "Epoch 612/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 613/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4091\n",
      "Epoch 614/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 615/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 616/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 617/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 618/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4091\n",
      "Epoch 619/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 620/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 621/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 622/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 623/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 624/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 625/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 626/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 627/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 628/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 629/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 630/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 631/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 632/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 633/10000\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.4091\n",
      "Epoch 634/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 635/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 636/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 637/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4091\n",
      "Epoch 638/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 639/10000\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 0.4091\n",
      "Epoch 640/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 641/10000\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 0.4091\n",
      "Epoch 642/10000\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 0.4091\n",
      "Epoch 643/10000\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 0.4091\n",
      "Epoch 644/10000\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 0.4091\n",
      "Epoch 645/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 646/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 647/10000\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 0.4091\n",
      "Epoch 648/10000\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 0.4091\n",
      "Epoch 649/10000\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.4091\n",
      "Epoch 650/10000\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 0.4091\n",
      "Epoch 651/10000\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 0.4091\n",
      "Epoch 652/10000\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 0.4091\n",
      "Epoch 653/10000\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 0.4091\n",
      "Epoch 654/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 655/10000\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 0.4091\n",
      "Epoch 656/10000\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 0.4091\n",
      "Epoch 657/10000\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 0.4091\n",
      "Epoch 658/10000\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 0.4091\n",
      "Epoch 659/10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-76f9a9dffeec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;31m#,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m           \u001b[0;31m#validation_split=0.2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m          )\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1207\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/rbenv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Note that `decoder_target_data` needs to be one-hot encoded,\n",
    "# rather than sequences of integers like `decoder_input_data`!\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs#,\n",
    "          #validation_split=0.2\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 42, 256)           18688     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 256), (None, 256) 525312    \n",
      "=================================================================\n",
      "Total params: 544,000\n",
      "Trainable params: 544,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 11)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 11, 256)      18688       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 11, 256), (N 525312      embedding_2[1][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2816)         0           lstm_2[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 11)           30987       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 574,987\n",
      "Trainable params: 574,987\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_mapped_input = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_mapped_input, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = Flatten()(decoder_outputs)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, max_decoder_seq_length), dtype='int32')\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = word_id_dict['\\t']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    i = 1\n",
    "    id_word_dict_cpy = id_word_dict.copy()\n",
    "    id_word_dict_cpy[0] = '0'\n",
    "    while not stop_condition:\n",
    "#         print(target_seq)\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "#         print(\"output_token:\", output_tokens)\n",
    "#         sampled_char = id_word_dict[int(output_tokens)]\n",
    "        decoded_sentence = str([id_word_dict_cpy[int(round(output_token))] for output_token in output_tokens[0]])\n",
    "        if i > 14:\n",
    "            stop_condition = True\n",
    "        else:\n",
    "            target_seq[0, 1:i] = output_tokens[0, 1:i]\n",
    "            i += 1\n",
    "#         # Exit condition: either hit max length\n",
    "#         # or find stop character.\n",
    "#         if (sampled_char == '\\n' or\n",
    "#            len(decoded_sentence) > max_decoder_seq_length):\n",
    "#             stop_condition = True\n",
    "\n",
    "#         # Update the target sequence (of length 1).\n",
    "#         target_seq[0, i] = output_tokens\n",
    "#         i += 1\n",
    "\n",
    "#         # Update states\n",
    "#         states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: indonesias finance minister said on monday the country will seek to host the headquarters for the asian infrastructure investment bank (aiib), a new multi-lateral lender that is led by china with beijing wanting the role for itself. \n",
      "\n",
      "Decoded sentence: ['sunday', 'new', '2', '\\n', 'beijing', 'on', 'fire', 'for', 'aiib', '0', '0']\n",
      "-\n",
      "Input sentence: the san jose earthquakes became the latest team in major league soccer to move into their own purpose-built venue and they celebrated the opening game at the avaya stadium on sunday with a 2-1 win over the chicago fire. \n",
      "\n",
      "Decoded sentence: ['win', 'compete', 'earthquakes', 'stadium', '\\n', 'win', '\\t', 'over', '\\n', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(num_samples):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
